{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo \n",
    "# import fitz  # PyMuPDF\n",
    "# import re\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer # import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "connect the DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('')\n",
    "db = client.sample_mflix\n",
    "collection = db.news\n",
    "print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf\n",
    "# !pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Function to preprocess text using NLTK\n",
    "def preprocess_text_nltk(text):\n",
    "    # Tokenize, remove stop words, and lemmatize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Function to extract keywords with TF-IDF and rank them\n",
    "def extract_keywords_from_folder(documents_db, num_keywords=10):\n",
    "    # Read and preprocess all PDF documents in the folder\n",
    "    documents = []\n",
    "    objectId = []\n",
    "    \n",
    "    inverted_index = defaultdict(list)\n",
    "    \n",
    "    for id, doc in enumerate(documents_db):\n",
    "        processed_text = preprocess_text_nltk(doc['plain_text'])\n",
    "        documents.append(processed_text)\n",
    "        objectId.append(doc['_id'])\n",
    "\n",
    "    \n",
    "    # Apply TF-IDF to the collection of documents\n",
    "\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_array = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Rank and display keywords with scores for each document\n",
    "    for i, doc in enumerate(documents):\n",
    "        tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "        top_indices = tfidf_scores.argsort()[::-1][:num_keywords]\n",
    "        for index in top_indices:\n",
    "            keyword = feature_array[index]\n",
    "            score = tfidf_scores[index]\n",
    "            inverted_index[keyword].append({'doc_id': objectId[i], 'score': score})\n",
    "    \n",
    "    for keyword, doc_list in inverted_index.items():\n",
    "        print(f\"\\nKeyword: {keyword}\")\n",
    "        for entry in doc_list:\n",
    "            print(f\"Document ID: {entry['doc_id']}, Score: {entry['score']:.4f}\")\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all documents in the 'news' collection\n",
    "documents_db = collection.find()\n",
    "\n",
    "# Iterate through the documents and print them\n",
    "\n",
    "inverted_idx_unsorted = extract_keywords_from_folder(documents_db, num_keywords=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer, 'vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_inverted_index_by_score(inverted_index):\n",
    "    \"\"\"\n",
    "    Sort the inverted index based on the document scores for each keyword.\n",
    "\n",
    "    Parameters:\n",
    "        inverted_index (dict): The inverted index containing keywords, document IDs, and scores.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new inverted index with sorted document entries for each keyword.\n",
    "    \"\"\"\n",
    "    updated_index = {}\n",
    "    for keyword, doc_list in inverted_index.items():\n",
    "        updated_doc_list = []\n",
    "        for entry in doc_list:\n",
    "            # Convert Document ID to string\n",
    "            updated_entry = {\n",
    "                'doc_id': str(entry['doc_id']),\n",
    "                'score': entry['score']\n",
    "            }\n",
    "            updated_doc_list.append(updated_entry)\n",
    "        updated_index[keyword] = updated_doc_list\n",
    "        \n",
    "    sorted_updated_index = {}\n",
    "    for keyword, doc_list in updated_index.items():\n",
    "        # Sort the document entries for the keyword by score in descending order\n",
    "        sorted_doc_list = sorted(doc_list, key=lambda x: x['score'], reverse=True)\n",
    "        sorted_updated_index[keyword] = sorted_doc_list\n",
    "    \n",
    "    file_path = 'inverted_index_output_sorted.json'\n",
    "    with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(sorted_updated_index, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return sorted_updated_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = sort_inverted_index_by_score(inverted_idx_unsorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query processing and searching in Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    \"\"\"Preprocess the query to match the terms used in the inverted index.\"\"\"\n",
    "    # Lowercase and remove special characters\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'\\W+', ' ', query)\n",
    "    return query.split()\n",
    "\n",
    "def extract_keywords_from_query(query, vectorizer, inverted_index, num_results=10):\n",
    "    \"\"\"\n",
    "    Extract keywords from a query, search the inverted index, and return relevant documents.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str): The user query.\n",
    "        vectorizer (TfidfVectorizer): The vectorizer used to build the inverted index.\n",
    "        inverted_index (dict): The inverted index containing keywords, document IDs, and scores.\n",
    "        num_results (int): Number of top results to return.\n",
    "    Returns:\n",
    "        List of relevant documents with scores.\n",
    "    \"\"\"\n",
    "    # Preprocess the query\n",
    "    processed_query = preprocess_query(query)\n",
    "    \n",
    "    # Vectorize the query to identify relevant keywords\n",
    "    query_vector = vectorizer.transform([' '.join(processed_query)]).toarray()[0]\n",
    "    feature_array = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Extract keywords with non-zero TF-IDF scores\n",
    "    query_keywords = {feature_array[i]: query_vector[i] for i in range(len(feature_array)) if query_vector[i] > 0}\n",
    "    \n",
    "    # Search in the inverted index\n",
    "    results = defaultdict(float)\n",
    "    for keyword, score in query_keywords.items():\n",
    "        if keyword in inverted_index:\n",
    "            for doc_entry in inverted_index[keyword]:\n",
    "                doc_id = str(doc_entry['doc_id'])\n",
    "                results[doc_id] += score * doc_entry['score']  # Combine query and document scores\n",
    "\n",
    "    # Sort results by relevance\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)[:num_results]\n",
    "    \n",
    "    return sorted_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"artificial intelligence and deep learning\"\n",
    "results = extract_keywords_from_query(query, vectorizer, sorted_index, num_results=5)\n",
    "\n",
    "# Display results\n",
    "print(\"Query Results:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"Document ID: {doc_id}, Relevance Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
